{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Art Generation with Neural Style Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only for Google Colab\n",
    "# !pip3 install -U torch==1.12+cu113 torchvision==0.13.0+cu113 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import models, io\n",
    "from torchvision.transforms import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We going to use a VGG-19 ConvNet with the pre-trained weights in the ImageNetV1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_weights = models.VGG11_Weights.IMAGENET1K_V1\n",
    "# vgg_weights = None\n",
    "vgg = models.vgg11(weights=vgg_weights)\n",
    "vgg_preprocess = vgg_weights.transforms()\n",
    "\n",
    "# Set to eval mode\n",
    "vgg = vgg.eval()\n",
    "# Freezze the weights\n",
    "for p in vgg.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# Change to use only the secuential model to extract features\n",
    "vgg = vgg.features\n",
    "vgg = vgg.to(device)\n",
    "\n",
    "# Compile as torch script for speedup\n",
    "# vgg = torch.jit.script(vgg)\n",
    "\n",
    "print(vgg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_cost(C, G):\n",
    "    c, h, w = G.shape\n",
    "    return 1 / (c * h * w) * torch.sum(torch.square(C - G))\n",
    "\n",
    "\n",
    "def gram_matrix(X):\n",
    "    return torch.matmul(X, X.T)\n",
    "\n",
    "\n",
    "def layer_style_cost(S, G):\n",
    "    c, h, w = G.shape\n",
    "\n",
    "    S = torch.reshape(S, shape=[c, -1])\n",
    "    G = torch.reshape(G, shape=[c, -1])\n",
    "\n",
    "    gram_S = gram_matrix(S)\n",
    "    gram_G = gram_matrix(G)\n",
    "    return 1 / (4 * (c * h * w) ** 2) * torch.sum(torch.square(gram_S - gram_G))\n",
    "\n",
    "\n",
    "def style_cost(S, G, weights):\n",
    "    sum = 0\n",
    "\n",
    "    for i, w in enumerate(weights):\n",
    "        cost = layer_style_cost(S[i], G[i])\n",
    "        sum += w * cost\n",
    "\n",
    "    return sum\n",
    "\n",
    "\n",
    "def total_cost(Jc, Js, alpha=10, beta=40):\n",
    "    return alpha * Jc + beta * Js\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PartialOutputsModule:\n",
    "    def __init__(self, module: nn.Module, layers):\n",
    "        super().__init__()\n",
    "        self.outputs = dict()\n",
    "        self.handlers = []\n",
    "        self.module = module\n",
    "        self.layers = layers\n",
    "\n",
    "        for layer in layers:\n",
    "            self.handlers.append(\n",
    "                module[layer].register_forward_hook(self.get_activation(layer))\n",
    "            )\n",
    "\n",
    "    def get_activation(self, name):\n",
    "        def hook(module, input, output):\n",
    "            self.outputs[name] = output\n",
    "\n",
    "        return hook\n",
    "\n",
    "    def unregister(self):\n",
    "        for handler in self.handlers:\n",
    "            handler.remove()\n",
    "\n",
    "    def __call__(self, x):\n",
    "        _ = self.module(x)\n",
    "        return [self.outputs[x] for x in self.layers]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_image = io.read_image(\"images/me.jpeg\")\n",
    "\n",
    "plt.imshow(content_image.permute(dims=[1, 2, 0]))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_image = io.read_image(\"images/van-gogh.jpg\")\n",
    "\n",
    "plt.imshow(style_image.permute(dims=[1, 2, 0]))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_image = torch.clone(content_image)\n",
    "# generated_image += (torch.rand(size=content_image.shape) * 20 - 10).type(torch.int)\n",
    "\n",
    "plt.imshow(generated_image.permute([1, 2, 0]))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get encoders for content and the style\n",
    "STYLE_LAYERS = {\n",
    "    1: 1,\n",
    "    4: 0.7,\n",
    "    9: 0.5,\n",
    "    14: 0.2,\n",
    "    19: 0.2,\n",
    "}\n",
    "CONTENT_LAYER = 19\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_image(img, crop_size=224, resize_size=(450, 600)):\n",
    "    img = F.resize(img, resize_size)\n",
    "    # img = F.center_crop(img, crop_size)\n",
    "    if not isinstance(img, torch.Tensor):\n",
    "        img = F.pil_to_tensor(img)\n",
    "    img = F.convert_image_dtype(img, torch.float)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_image = transform_image(content_image)\n",
    "style_image = transform_image(style_image)\n",
    "generated_image = transform_image(generated_image)\n",
    "\n",
    "# Add noise to the generated image\n",
    "# generated_image += torch.randn(generated_image.size())\n",
    "# generated_image = torch.clamp(generated_image, 0, 255)\n",
    "\n",
    "content_image = content_image.to(device)\n",
    "style_image = style_image.to(device)\n",
    "generated_image = generated_image.to(device)\n",
    "\n",
    "generated_image = nn.Parameter(generated_image, requires_grad=True)\n",
    "optimizer = torch.optim.Adam([generated_image], lr=0.01)\n",
    "epochs = 10000\n",
    "\n",
    "partial = PartialOutputsModule(vgg, list(STYLE_LAYERS.keys()) + [CONTENT_LAYER])\n",
    "C = partial(content_image)[-1]\n",
    "S = partial(style_image)[: len(STYLE_LAYERS)]\n",
    "\n",
    "# Train our image\n",
    "for i in range(epochs):\n",
    "    outputs = partial(generated_image)\n",
    "    Js = style_cost(S, outputs[: len(STYLE_LAYERS)], list(STYLE_LAYERS.values()))\n",
    "    Jc = content_cost(C, outputs[-1])\n",
    "    loss = total_cost(Jc, Js, alpha=10, beta=80)\n",
    "\n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Epoch {i}: Loss {loss.item()}\")\n",
    "        with torch.no_grad():\n",
    "            Y = generated_image.cpu()\n",
    "            Y = torch.clamp(Y, 0.0, 1.0)\n",
    "            plt.imshow(Y.permute([1, 2, 0]))\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final image encoded as png\n",
    "with torch.no_grad():\n",
    "    Y = generated_image.cpu()\n",
    "    Y = torch.clamp(Y, 0.0, 1.0)\n",
    "    io.save_image(Y, \"images/generated.png\")\n",
    "    print(\"Saved image\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
